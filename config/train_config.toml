
batch_size = 128
num_epochs = 6
grad_clip_norm = 1.0
grad_accumulation_steps = 1
l2_emb = 0.0

num_workers = 0
pin_memory = false


use_amp = false         
amp_init_scale = 65536.0     
amp_growth_factor = 2.0        
amp_backoff_factor = 0.5         
amp_growth_interval = 2000    

[optimizer]
# type = "adamw"
type = "adamw"
lr = 0.01
betas = [0.9, 0.98]
weight_decay = 1e-4
eps = 1e-8


[scheduler]
act = true
type = "cosine_with_warmup_restarts"  
# type = 'constant'
warmup_steps = 1500
min_lr_ratio = 0.3
T_0 = 5000  
T_mult = 2   
eta_min_ratio = 0.05
restart_ratio = 0.1 
init_lr_ratio = 0.5


[logging]
log_grad_freq = 10
log_train_freq = 10
save_model_freq = 1  # 每多少个epoch保存一次模型



[data]
num_workers = 0
shuffle_train = true
shuffle_valid = false

[eval]
acc_k = [1, 3]
distance = "cosine"
